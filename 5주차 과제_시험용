# 필요한 라이브러리 불러오기
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 데이터 정의 (50개의 x, y 값 직접 입력)
# 실제 과제에서는 CSV 파일을 GitHub에서 읽어오도록 되어 있음
X = np.array([...])  # x 값들 (50개)
y = np.array([...])  # y 값들 (50개)

# 데이터 분포 시각화
plt.figure(figsize=(10, 6))
plt.scatter(X, y, color='blue', alpha=0.7)
plt.title('Data Distribution')
plt.xlabel('x')
plt.ylabel('y')
plt.grid(True, linestyle='--', alpha=0.7)
plt.show()

# -------------------------
# 1. 최소제곱법 (Least Squares Method)
# -------------------------

# 1-1. 선형 예측 모델을 위한 X 행렬 구성 (bias 항 추가)
X_b = np.c_[np.ones((len(X), 1)), X]  # [1, x] 형태로 구성

# 최소제곱법 공식: theta = (X^T * X)^-1 * X^T * y
theta_linear = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)

# 선형 모델 파라미터 출력
linear_intercept = theta_linear[0]
linear_coef = theta_linear[1]
print("1-1. Linear Prediction Model using Least Squares Method")
print(f"Parameters: y = {linear_coef:.4f}x + {linear_intercept:.4f}")

# 선형 모델 시각화
plt.figure(figsize=(10, 6))
plt.scatter(X, y, color='blue', alpha=0.7, label='Data')
x_range = np.linspace(min(X)-0.1, max(X)+0.1, 100)  # 예측용 x 범위
plt.plot(x_range, linear_coef * x_range + linear_intercept, 'r-', linewidth=2,
         label=f'Linear Model: y = {linear_coef:.4f}x + {linear_intercept:.4f}')
plt.title('Linear Prediction Model using Least Squares Method')
plt.xlabel('x')
plt.ylabel('y')
plt.grid(True, linestyle='--', alpha=0.7)
plt.legend()
plt.show()

# 1-2. 2차 다항 회귀 (x^2 항 포함)
X_poly2 = np.column_stack((np.ones(len(X)), X, X**2))  # [1, x, x^2]
theta_poly2 = np.linalg.inv(X_poly2.T.dot(X_poly2)).dot(X_poly2.T).dot(y)
print("1-2. 2nd Order Polynomial Prediction Model using Least Squares Method")
print(f"Parameters: y = {theta_poly2[2]:.4f}x^2 + {theta_poly2[1]:.4f}x + {theta_poly2[0]:.4f}")

# 10차 다항 회귀
X_poly10 = np.zeros((len(X), 11))  # [1, x, x^2, ..., x^10]
for i in range(11):
    X_poly10[:, i] = X**i

theta_poly10 = np.linalg.inv(X_poly10.T.dot(X_poly10)).dot(X_poly10.T).dot(y)
print("1-2. 10th Order Polynomial Prediction Model using Least Squares Method")
coef_str = f"Parameters: y = {theta_poly10[0]:.4f}"
for i in range(1, 11):
    coef_str += f" + {theta_poly10[i]:.4f}x^{i}"
print(coef_str)

# 다항 모델 시각화 (2차, 10차)
plt.figure(figsize=(10, 6))
plt.scatter(X, y, color='blue', alpha=0.7, label='Data')

# 선형 모델
plt.plot(x_range, linear_coef * x_range + linear_intercept, 'r-', linewidth=2, label='Linear Model')

# 2차 모델 예측
y_pred_poly2 = np.zeros_like(x_range)
for i in range(3):
    y_pred_poly2 += theta_poly2[i] * x_range**i
plt.plot(x_range, y_pred_poly2, 'g-', linewidth=2, label='2nd Order Model')

# 10차 모델 예측
y_pred_poly10 = np.zeros_like(x_range)
for i in range(11):
    y_pred_poly10 += theta_poly10[i] * x_range**i
plt.plot(x_range, y_pred_poly10, 'y-', linewidth=2, label='10th Order Model')

plt.title('Prediction Models Comparison using Least Squares Method')
plt.xlabel('x')
plt.ylabel('y')
plt.grid(True, linestyle='--', alpha=0.7)
plt.legend()
plt.show()

# -------------------------
# 2. 경사하강법 (Gradient Descent)
# -------------------------

# 경사하강법 함수 정의
def gradient_descent(X, y, learning_rate=0.001, epochs=500):
    m = len(y)
    theta = np.zeros(X.shape[1])  # 초기 파라미터 0
    cost_history = []  # 비용 저장용

    for i in range(epochs):
        prediction = X.dot(theta)
        error = prediction - y
        gradient = X.T.dot(error) / m
        theta -= learning_rate * gradient
        cost = np.sum(error ** 2) / (2 * m)
        cost_history.append(cost)

    return theta, cost_history

# 2-1. 선형 회귀
X_linear = np.column_stack((np.ones(len(X)), X))  # [1, x]
theta_gd_linear, cost_history_linear = gradient_descent(X_linear, y)

print("2-1. Linear Prediction Model using Gradient Descent")
print(f"Parameters: y = {theta_gd_linear[1]:.4f}x + {theta_gd_linear[0]:.4f}")

# 시각화
plt.figure(figsize=(10, 6))
plt.scatter(X, y, color='blue', alpha=0.7, label='Data')
plt.plot(x_range, theta_gd_linear[1] * x_range + theta_gd_linear[0], 'r-', linewidth=2,
         label=f'GD Linear Model: y = {theta_gd_linear[1]:.4f}x + {theta_gd_linear[0]:.4f}')
plt.title('Linear Prediction Model using Gradient Descent')
plt.xlabel('x')
plt.ylabel('y')
plt.grid(True, linestyle='--', alpha=0.7)
plt.legend()
plt.show()

# 비용 함수 시각화
plt.figure(figsize=(10, 6))
plt.plot(range(500), cost_history_linear)
plt.title('Linear Model Training Progress using Gradient Descent')
plt.xlabel('Epoch')
plt.ylabel('Cost')
plt.grid(True, linestyle='--', alpha=0.7)
plt.show()

# 2-2. 비선형 회귀 (2차, 4차)
X_poly2_features = np.column_stack((np.ones(len(X)), X, X**2))
theta_gd_poly2, cost_history_poly2 = gradient_descent(X_poly2_features, y)

print("2-2. 2nd Order Polynomial Prediction Model using Gradient Descent")
print(f"Parameters: y = {theta_gd_poly2[2]:.4f}x^2 + {theta_gd_poly2[1]:.4f}x + {theta_gd_poly2[0]:.4f}")

X_poly4_features = np.column_stack((np.ones(len(X)), X, X**2, X**3, X**4))
theta_gd_poly4, cost_history_poly4 = gradient_descent(X_poly4_features, y)

print("2-2. 4th Order Polynomial Prediction Model using Gradient Descent")
print(f"Parameters: y = {theta_gd_poly4[4]:.4f}x^4 + {theta_gd_poly4[3]:.4f}x^3 + {theta_gd_poly4[2]:.4f}x^2 + {theta_gd_poly4[1]:.4f}x + {theta_gd_poly4[0]:.4f}")

# 다항 모델 시각화
plt.figure(figsize=(10, 6))
plt.scatter(X, y, color='blue', alpha=0.7, label='Data')

# 선형
plt.plot(x_range, theta_gd_linear[1] * x_range + theta_gd_linear[0], 'r-', linewidth=2, label='GD Linear Model')

# 2차
y_pred_gd_poly2 = theta_gd_poly2[0] + theta_gd_poly2[1] * x_range + theta_gd_poly2[2] * x_range**2
plt.plot(x_range, y_pred_gd_poly2, 'g-', linewidth=2, label='GD 2nd Order Model')

# 4차
y_pred_gd_poly4 = sum(theta_gd_poly4[i] * x_range**i for i in range(5))
plt.plot(x_range, y_pred_gd_poly4, 'y-', linewidth=2, label='GD 4th Order Model')

plt.title('Prediction Models Comparison using Gradient Descent')
plt.xlabel('x')
plt.ylabel('y')
plt.grid(True, linestyle='--', alpha=0.7)
plt.legend()
plt.show()

# 비용 함수 비교 시각화
plt.figure(figsize=(10, 6))
plt.plot(range(500), cost_history_linear, 'r-', label='Linear Model')
plt.plot(range(500), cost_history_poly2, 'g-', label='2nd Order Model')
plt.plot(range(500), cost_history_poly4, 'y-', label='4th Order Model')
plt.title('Training Progress Comparison of Models using Gradient Descent')
plt.xlabel('Epoch')
plt.ylabel('Cost')
plt.grid(True, linestyle='--', alpha=0.7)
plt.legend()
plt.show()

# -------------------------
# 3. 파라미터 값 비교
# -------------------------
print("\n3. 두 방법의 파라미터 값 비교")

print("======== 선형 모델 파라미터 비교 ========")
print(f"최소제곱법: y = {linear_coef:.4f}x + {linear_intercept:.4f}")
print(f"경사하강법: y = {theta_gd_linear[1]:.4f}x + {theta_gd_linear[0]:.4f}")

print("\n======== 2차 모델 파라미터 비교 ========")
print(f"최소제곱법: y = {theta_poly2[2]:.4f}x^2 + {theta_poly2[1]:.4f}x + {theta_poly2[0]:.4f}")
print(f"경사하강법: y = {theta_gd_poly2[2]:.4f}x^2 + {theta_gd_poly2[1]:.4f}x + {theta_gd_poly2[0]:.4f}")
